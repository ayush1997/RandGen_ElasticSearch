{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code for ** Random Data Generation** and it's storage to **ElasticSearch**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import datetime \n",
    "import uuid\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# connect to our cluster\n",
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch([{'host': 'localhost', 'port': 9200}])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Random Data Size, 1 million\n",
    "\n",
    "dataset_size = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Gaussian distribution of #Bytes\n",
    "# The No.of transfered are Random Gaussian Distributions around 2GB,5GB and 8GB.\n",
    "\n",
    "mu1, sigma1 = 2000000000, 350000000\n",
    "mu2, sigma2 = 5000000000, 300000000\n",
    "mu3, sigma3 = 8000000000, 350000000\n",
    "\n",
    "size = list(np.random.normal(mu1, sigma1, 330000))+list(np.random.normal(mu2, sigma2, 330000))+list(np.random.normal(mu3, sigma3, 340000))\n",
    "random.shuffle(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SOURCE SITE GENERATION\n",
    "# It is assumed that  site A is \"better\" and has 40% of transfers foloowed by B(30%), C(10%),D(10%),E(10%) \n",
    "\n",
    "source_site = [\"A\"]*int(0.40*dataset_size)+[\"B\"]*int(0.30*dataset_size)+[\"C\"]*int(0.10*dataset_size)+[\"D\"]*int(0.10*dataset_size)+[\"E\"]*int(0.10*dataset_size)\n",
    "\n",
    "random.shuffle(source_site)\n",
    "\n",
    "def src_site(i):\n",
    "    return source_site[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DESTINATION SITE GENERATION\n",
    "\n",
    "def dst_site(site):\n",
    "    src = [\"A\",\"B\",\"C\",\"D\",\"E\"]\n",
    "    src.remove(site)\n",
    "    return np.random.choice(src,p=[0.4, 0.3, 0.2, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate Timestamp start ---- Taking start time fixed as 08/03/2017-10:00\n",
    "\n",
    "startDate = datetime.datetime(2017, 3, 8,10,00)\n",
    "def timestamp_start():\n",
    "    return startDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate Timestamp end  \n",
    "#Assuming 1sec transfer time for 10MB Data, for linear dependence of time duration with #Bytes\n",
    "\n",
    "\n",
    "def timestamp_end(bytes):\n",
    "    return startDate + datetime.timedelta(seconds=(bytes/(10240.0*1024.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate UUID\n",
    "\n",
    "def generate_uuid():\n",
    "    return str(uuid.uuid4())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get final_event_type, keeping 95% of success and 5% Failure\n",
    "\n",
    "count = {\"transfer-success\": 0,\n",
    "\"transfer-failed\" : 0}\n",
    "def get_final_event_type():\n",
    "    events = [\"transfer-success\",\"transfer-failed\"]\n",
    "    if count[\"transfer-success\"] >= (0.95*(dataset_size)):\n",
    "        count[\"transfer-failed\"] +=1\n",
    "        return \"transfer-failed\"\n",
    "    \n",
    "    count[\"transfer-success\"] +=1\n",
    "    return \"transfer_success\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " response: '{u'acknowledged': True}'\n",
      "creating 'atlas' index...\n",
      "Transfer to elasticsearch started .....\n"
     ]
    }
   ],
   "source": [
    "# Random Dataset Generator\n",
    "# This process takes around 8-9 Mins (will vary depending of machine)\n",
    "\n",
    "INDEX_NAME = 'atlas'\n",
    "op_dict_1 = {\n",
    "        \"index\": {\n",
    "            \"_index\": 'atlas', \n",
    "            \"_type\": 'start'\n",
    "            \n",
    "        }\n",
    "    }\n",
    "op_dict_2 = {\n",
    "        \"index\": {\n",
    "            \"_index\": 'atlas', \n",
    "            \"_type\": 'end'\n",
    "            \n",
    "        }\n",
    "    }\n",
    "request_body = {\n",
    "    \"settings\" : {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0,\n",
    "        \"refresh_interval\": \"30s\"\n",
    "    }\n",
    "}\n",
    "\n",
    "bulk_start = []\n",
    "bulk_end=[]\n",
    "if es.indices.exists('atlas'):\n",
    "\n",
    "    res = es.indices.delete(index = 'atlas')\n",
    "    print(\" response: '%s'\" % (res))\n",
    "\n",
    "print(\"creating '%s' index...\" % (INDEX_NAME))\n",
    "res = es.indices.create(index = 'atlas', body = request_body)\n",
    "\n",
    "start_time = time.time()\n",
    "print \"Transfer to elasticsearch started .....\"\n",
    "for i in range(dataset_size):\n",
    "    event={}\n",
    "\n",
    "    transfer_src = src_site(i)\n",
    "    transfer_dst = dst_site(transfer_src)\n",
    "    transfer_starttime = startDate\n",
    "    event = {\"event_type\": 'transfer-queued',\n",
    "                       \"uuid\": generate_uuid(),\n",
    "                       \"bytes\": size[i],\n",
    "                       \"src_site\": transfer_src,\n",
    "                       \"dst_site\": transfer_dst,\n",
    "                       \"timestamp\": str(transfer_starttime)}\n",
    "\n",
    "    \n",
    "    bulk_start.append(op_dict_1)\n",
    "    bulk_start.append(event)\n",
    "\n",
    "    event_new = {}\n",
    "    event_new[\"event_type\"] = event[\"event_type\"]\n",
    "    event_new[\"uuid\"] = event[\"uuid\"]\n",
    "    event_new[\"bytes\"] = event[\"bytes\"]\n",
    "    event_new[\"src_site\"] = event[\"src_site\"]\n",
    "    event_new[\"dst_site\"] = event[\"dst_site\"]\n",
    "    \n",
    "    event_type = get_final_event_type()\n",
    "    event_new[\"event_type\"] = event_type\n",
    "    event_new[\"timestamp\"] = str(timestamp_end(event[\"bytes\"]))\n",
    "\n",
    "    bulk_end.append(op_dict_2)\n",
    "    bulk_end.append(event_new)\n",
    "\n",
    "    if len(bulk_start) == 2500:\n",
    "            res = es.bulk(index = 'atlas', body = bulk_start, refresh = True)\n",
    "            bulk_start=[]\n",
    "\n",
    "    if len(bulk_end) == 2500:\n",
    "            res = es.bulk(index = 'atlas', body = bulk_end, refresh = True)\n",
    "            bulk_end=[]\n",
    "\n",
    "print \"Transfer completed....\"            \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
